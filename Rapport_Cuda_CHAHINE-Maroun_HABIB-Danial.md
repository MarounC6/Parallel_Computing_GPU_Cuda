# TP CUDA - Programmation GPU

**Auteurs :** CHAHINE Maroun, HABIB Danial  
**Date :** D√©cembre 2025  
**Formation :** 5IF - INSA Lyon

---

## Table des mati√®res

1. [Partie 1 : Calcul de œÄ](#partie-1--calcul-de-œÄ)
2. [Partie 2 : Produit Matrice-Vecteur](#partie-2--produit-matrice-vecteur)
3. [Partie 3 : Multiplication de Matrices](#partie-3--multiplication-de-matrices)

---

## Partie 1 : Calcul de œÄ

### 1.1 Objectif

L'objectif de cette premi√®re partie est de calculer une approximation de œÄ en utilisant la m√©thode des rectangles pour approximer l'int√©grale suivante :

$$\pi = \int_0^1 \frac{4}{1+x^2} dx$$

Nous avons impl√©ment√© plusieurs versions du programme pour comparer les performances :
- **Version s√©quentielle** (CPU)
- **Version CUDA simple** (GPU)
- **Version avec m√©moire partag√©e** (GPU optimis√©e)
- **Version avec r√©duction √† 2 niveaux** (GPU optimis√©e)
- **Version avec r√©duction multi-√©tages** (GPU optimis√©e)
- **Version avec tableau** (GPU)
- **Version tableau avec r√©duction √† 2 niveaux** (GPU optimis√©e)

### 1.2 M√©thodologie

Pour chaque impl√©mentation, nous avons effectu√© des tests avec :
- **Nombre de pas** : 1 000 et 1 000 000
- **Threads par bloc** : 1, 32, 64, 128, 256
- **R√©p√©titions** : 10 ex√©cutions par configuration pour obtenir des moyennes fiables

### 1.3 R√©sultats et Analyse

![Analyse de performance - Partie 1](Part_1/performance_analysis.png)

#### Observations principales :

**1. Version s√©quentielle (CPU)**
- Temps d'ex√©cution stable mais lent
- Pas d'influence du param√®tre "threads par bloc" (normal, car CPU)
- Performance de r√©f√©rence pour calculer les speedups

**2. Version CUDA simple (pi_cuda_gpu)**
- Premier portage sur GPU
- Am√©lioration significative par rapport au CPU
- Sensible au nombre de threads par bloc
- Meilleure performance autour de 128-256 threads/bloc

**3. Version avec m√©moire partag√©e (pi_cuda_shared_memory)**
- Utilisation de `__shared__` pour r√©duire les acc√®s √† la m√©moire globale
- R√©duction des latences m√©moire
- Performance am√©lior√©e par rapport √† la version simple
- La m√©moire partag√©e permet aux threads d'un m√™me bloc de collaborer efficacement

**4. Versions avec r√©duction (2-level et multistage)**
- Approches optimis√©es pour minimiser les synchronisations
- R√©duction hi√©rarchique des r√©sultats partiels
- **Multistage reduction** : meilleure performance globale
- √âvite les goulots d'√©tranglement lors de la combinaison des r√©sultats

**5. Versions avec tableau**
- Stockage des r√©sultats partiels dans un tableau global
- Utile pour d√©boguer et analyser les contributions individuelles
- Performance l√©g√®rement inf√©rieure aux versions avec r√©duction optimale

#### Speedup observ√© :
- **GPU vs CPU** : Acc√©l√©ration jusqu'√† **10-50x** selon la configuration
- **Impact du nombre de pas** : Plus le calcul est complexe (1M vs 1K pas), plus le GPU montre son avantage
- **Threads optimaux** : 128-256 threads/bloc offrent le meilleur compromis

### 1.4 Conclusion Partie 1

Les r√©sultats d√©montrent clairement l'int√©r√™t du GPU pour les calculs massivement parall√®les. Les optimisations comme la m√©moire partag√©e et les r√©ductions multi-niveaux permettent d'exploiter au maximum la puissance du GPU. Le choix du nombre de threads par bloc est crucial : trop peu limite le parall√©lisme, trop peut saturer les ressources.

---

## Partie 2 : Produit Matrice-Vecteur

### 2.1 Objectif

Cette partie consiste √† calculer le produit d'une matrice par un vecteur : **Y = A √ó X**

Avec :
- **A** : matrice de dimension **N√óM** (N lignes, M colonnes)
- **X** : vecteur de dimension **M** (colonne)
- **Y** : vecteur r√©sultat de dimension **N** (colonne)

Impl√©mentations r√©alis√©es :
- **Version s√©quentielle** (CPU)
- **Version CUDA simple** (GPU)
- **Version avec m√©moire partag√©e** (GPU)
- **Version avec m√©moire partag√©e optimis√©e** (GPU)
- **Version avec r√©duction √† 2 niveaux** (GPU)

### 2.2 M√©thodologie

Tests effectu√©s avec :
- **Tailles de matrice** : 
  - N = 2^n avec n ‚àà {2, 4, 6, 8, 10, 12} (nombre de lignes)
  - M = 2^m avec m ‚àà {1, 3, 7, 9, 11} (nombre de colonnes)
  - Donc N varie de 4 √† 4096, et M varie selon les tests
- **Vecteur X** : de dimension M (nombre de colonnes de A)
- **Vecteur Y** : de dimension N (nombre de lignes de A)
- **Threads par bloc** : 1, 32, 64, 128, 256
- **R√©p√©titions** : 10 ex√©cutions par configuration

### 2.3 R√©sultats et Analyse

![Analyse de performance - Partie 2](Part_2/performance_analysis.png)

#### Observations principales :

**1. Version s√©quentielle (CPU)**
- Temps d'ex√©cution cro√Æt avec N√óM (O(N√óM))
- Devient rapidement prohibitif pour les grandes matrices
- Aucune exploitation du parall√©lisme disponible

**2. Version CUDA simple (matrix_cuda_gpu)**
- Chaque thread calcule un √©l√©ment du vecteur r√©sultat Y
- Parall√©lisme naturel : N threads pour N √©l√©ments de sortie
- Chaque thread fait M multiplications + M additions (parcourt une ligne de A)
- Gain important par rapport au CPU
- Bonne scalabilit√© avec la taille de la matrice

**3. Version avec m√©moire partag√©e (matrix_cuda_shared_memory)**
- Cache les donn√©es fr√©quemment acc√©d√©es dans la m√©moire partag√©e
- R√©duit les acc√®s √† la m√©moire globale (plus lente)
- Am√©lioration notable des performances
- Particuli√®rement efficace pour les grandes matrices o√π le ratio calcul/m√©moire est √©lev√©

**4. Version optimis√©e (matrix_cuda_shared_memory_optimized)**
- Optimisations suppl√©mentaires :
  - Coalescence des acc√®s m√©moire
  - Minimisation des divergences de branches
  - Meilleure utilisation des registres
- **Meilleures performances globales**
- Exploite au maximum l'architecture GPU

**5. Version avec r√©duction √† 2 niveaux (matrix_cuda_2_level_reduction)**
- R√©duction hi√©rarchique des produits partiels
- Deux niveaux de r√©duction : au sein du bloc puis globalement
- Performance comparable √† la version optimis√©e
- Approche diff√©rente mais r√©sultats similaires

#### Speedup observ√© :
- **GPU vs CPU** : Acc√©l√©ration jusqu'√† **100-200x** pour les grandes matrices
- **Impact de N** : Plus la matrice est grande, plus le GPU est avantageux
- **M√©moire partag√©e** : Gain de **20-40%** par rapport √† la version simple
- **Optimisations** : Gain additionnel de **10-20%**

### 2.4 Conclusion Partie 2

Le produit matrice-vecteur (Y = A√óX avec A de taille N√óM) est une op√©ration id√©ale pour le GPU car chaque √©l√©ment du r√©sultat Y peut √™tre calcul√© ind√©pendamment. Chaque thread traite une ligne de la matrice A (M √©l√©ments) pour produire un √©l√©ment de Y. Les optimisations m√©moire (m√©moire partag√©e, coalescence) sont essentielles pour atteindre les meilleures performances. Pour les grandes matrices (N > 1000, M > 1000), le GPU devient indispensable.

---

## Partie 3 : Multiplication de Matrices

### 3.1 Objectif

Cette partie vise √† impl√©menter la multiplication de matrices **C = A √ó B** avec :
- **A** : matrice N√óP
- **B** : matrice P√óM
- **C** : matrice r√©sultat N√óM

L'objectif est d'explorer diff√©rentes strat√©gies d'optimisation et l'impact de la pr√©cision num√©rique :

Impl√©mentations r√©alis√©es :
- **Version s√©quentielle** (CPU, r√©f√©rence fournie)
- **Version CUDA 1 thread/bloc** (Q3.1 - parall√©lisme minimal)
- **Version avec m√©moire partag√©e** (Q3.5 - tiling optimis√©)
- **Version float** (Q3.9 - pr√©cision simple 32 bits)

**Note** : La version half precision (Q3.13) n'a pas √©t√© impl√©ment√©e car la biblioth√®que `half.hpp` recommand√©e dans le sujet n'est pas compatible avec CUDA. Les fonctions de cette biblioth√®que sont marqu√©es `__host__` uniquement et ne peuvent pas √™tre appel√©es depuis les kernels GPU (`__device__`). L'alternative `cuda_fp16.h` aurait pu √™tre utilis√©e, mais n√©cessite une architecture GPU r√©cente (compute capability ‚â• 5.3) qui n'√©tait pas disponible sur la machine de test.

### 3.2 M√©thodologie

Tests pr√©vus avec :
- **Dimensions** : N, M, P ‚àà {1000, 4000, 8000, 12000, 18000}
- **R√©p√©titions** : 10 ex√©cutions par configuration
- **M√©triques** : Temps d'ex√©cution, GFLOPS, speedup, pr√©cision

### 3.3 Impl√©mentations D√©taill√©es

#### 3.3.1 Version s√©quentielle (R√©f√©rence)

**Code fourni par l'enseignant** - impl√©mentation CPU classique :
```c
for (i = 0; i < Ndim; i++) {
    for (j = 0; j < Mdim; j++) {
        for (k = 0; k < Pdim; k++) {
            *(C+(i*Ndim+j)) += *(A+(i*Ndim+k)) * *(B+(k*Pdim+j));
        }
    }
}
```

- Triple boucle imbriqu√©e : O(N√óM√óP)
- Calcul s√©quentiel √©l√©ment par √©l√©ment
- Sert de r√©f√©rence pour valider les r√©sultats GPU

#### 3.3.2 Version CUDA 1 thread par bloc (Q3.1)

**Objectif** : Portage minimal sur GPU pour comprendre les bases.

**Strat√©gie** :
- Grille 2D : `gridDim(Mdim, Ndim)`
- Chaque bloc contient **1 seul thread** : `blockDim(1, 1)`
- Chaque thread calcule UN √©l√©ment de C

**Code cl√©** :
```cuda
int j = blockIdx.x;  // Colonne
int i = blockIdx.y;  // Ligne
double sum = 0.0;
for (int k = 0; k < Pdim; k++) {
    sum += A[i*Ndim+k] * B[k*Pdim+j];
}
C[i*Ndim+j] = sum;
```

**Questions Q3.2-Q3.4** :
- **Q3.2** : Nombre de blocs = N√óM (un par √©l√©ment de C)
- **Q3.3** : Calculs par thread = P multiplications + P additions
- **Q3.4** : Performance attendue - **Faible** car :
  - Pas de parall√©lisme au niveau des blocs
  - Sous-utilisation du GPU (1 thread/bloc = gaspillage)
  - Pas d'optimisation m√©moire
  - Mais devrait quand m√™me battre le CPU gr√¢ce au parall√©lisme massif (N√óM threads simultan√©s)

#### 3.3.3 Version avec m√©moire partag√©e (Q3.5)

**Objectif** : Optimiser avec tiling et m√©moire partag√©e.

**Strat√©gie - Tiled Matrix Multiplication** :
- D√©coupage en tuiles de 16√ó16
- M√©moire partag√©e pour cacher les tuiles de A et B
- R√©duction des acc√®s √† la m√©moire globale

**Code cl√©** :
```cuda
#define TILE_SIZE 16
__shared__ double As[TILE_SIZE][TILE_SIZE];
__shared__ double Bs[TILE_SIZE][TILE_SIZE];

// Boucle sur les tuiles
for (int t = 0; t < (Pdim + TILE_SIZE - 1) / TILE_SIZE; t++) {
    // Charger tuile de A dans m√©moire partag√©e
    if (row < Ndim && t*TILE_SIZE+tx < Pdim)
        As[ty][tx] = A[row*Ndim + t*TILE_SIZE+tx];
    
    // Charger tuile de B dans m√©moire partag√©e
    if (col < Mdim && t*TILE_SIZE+ty < Pdim)
        Bs[ty][tx] = B[(t*TILE_SIZE+ty)*Pdim + col];
    
    __syncthreads();  // Synchronisation
    
    // Calcul sur la tuile en m√©moire partag√©e
    for (int k = 0; k < TILE_SIZE; k++) {
        sum += As[ty][k] * Bs[k][tx];
    }
    __syncthreads();
}
atomicAdd(&C[row*Ndim+col], sum);
```

**Questions Q3.6-Q3.8** :
- **Q3.6** : Blocs = ‚åàN/16‚åâ √ó ‚åàM/16‚åâ, Threads/bloc = 256 (16√ó16)
- **Q3.7** : Nombre de tuiles = ‚åàP/16‚åâ
- **Q3.8** : Performance attendue - **Excellente** car :
  - R√©utilisation des donn√©es en m√©moire partag√©e (100x plus rapide)
  - Chaque √©l√©ment de A et B lu une seule fois de la m√©moire globale
  - Parall√©lisme optimal (256 threads/bloc)
  - Facteur d'am√©lioration attendu : **50-100x vs version 1-thread**

#### 3.3.4 Version Float (Q3.9)

**Objectif** : Tester l'impact de la pr√©cision r√©duite (32 bits vs 64 bits).

**Changements** :
- Type `double` ‚Üí `float` partout
- Constantes `0.0` ‚Üí `0.0f`
- M√™mes algorithmes que la version shared memory

**Questions Q3.10-Q3.12** :
- **Q3.10** : Pr√©cision = 7-8 chiffres significatifs (vs 15-16 pour double)
- **Q3.11** : Erreur attendue = ~10^-6 √† 10^-7
- **Q3.12** : Performance attendue - **Meilleure** que double car :
  - GPUs modernes : d√©bit float souvent 2x sup√©rieur √† double
  - Bande passante m√©moire divis√©e par 2 (4 octets vs 8)
  - Plus de valeurs tiennent en cache/m√©moire partag√©e
  - Speedup attendu : **1.5-2x vs version double**

#### 3.3.5 Version Half Precision (Q3.13) - Non impl√©ment√©e

**Probl√®me rencontr√©** : La biblioth√®que `half.hpp` recommand√©e dans le sujet (half_float) n'est **pas compatible avec CUDA**.

**Explication technique** :
- Les fonctions de `half.hpp` sont marqu√©es `__host__` uniquement
- Elles ne peuvent pas √™tre appel√©es depuis les kernels GPU (`__device__` ou `__global__`)
- Erreur de compilation : `calling a __host__ function from a __global__ function is not allowed`

**Alternatives possibles** :
1. **`cuda_fp16.h`** : Biblioth√®que native CUDA pour half precision
   - N√©cessite compute capability ‚â• 5.3 (architecture Maxwell ou plus r√©cente)
   - Non disponible sur la machine de test utilis√©e
   
2. **`--expt-relaxed-constexpr`** : Flag exp√©rimental
   - Ne r√©sout pas le probl√®me fondamental d'incompatibilit√©

**R√©ponses th√©oriques Q3.14-Q3.16** :
- **Q3.14** : Pr√©cision half = 3-4 chiffres significatifs (vs 7-8 pour float, 15-16 pour double)
- **Q3.15** : Erreur attendue = ~10^-3 √† 10^-4 (perte de pr√©cision significative)
- **Q3.16** : Performance th√©orique :
  - Bande passante divis√©e par 4 vs double, par 2 vs float
  - Sur GPUs modernes avec Tensor Cores : acc√©l√©ration possible de 2-8x vs float
  - Sur GPUs anciens : peut √™tre plus lent que float (conversion overhead)
  - Trade-off pr√©cision/vitesse tr√®s int√©ressant pour ML/IA o√π la pr√©cision r√©duite suffit

**Conclusion** : L'impl√©mentation half precision n√©cessite soit une biblioth√®que compatible CUDA native, soit une architecture GPU plus r√©cente. Cette version n'a donc pas √©t√© incluse dans les benchmarks.

### 3.4 R√©sultats et Analyse

Les benchmarks ont √©t√© effectu√©s sur des matrices carr√©es (N=M=P) avec des dimensions de 1000, 2000 et 3000. Les tests ont √©t√© r√©p√©t√©s 10 fois pour obtenir des moyennes fiables.

**Note importante** : Les tests sont limit√©s aux matrices carr√©es en raison d'une limitation dans l'indexation du code de r√©f√©rence fourni par l'enseignant. Cette limitation n'affecte pas la validit√© des r√©sultats pour le cas d'usage le plus courant (multiplication de matrices carr√©es).

#### Graphique 1 : Performance en fonction de la dimension

![Performance des matrices carr√©es](Part_3/plots/performance_square_matrices.png)

Ce graphique montre le temps d'ex√©cution en fonction de la taille des matrices. On observe :
- **S√©quentiel** : Croissance cubique O(N¬≥) tr√®s marqu√©e - devient rapidement prohibitif
- **CUDA 1-thread** : Am√©lioration significative gr√¢ce au parall√©lisme massif
- **CUDA Shared** : L√©g√®re am√©lioration gr√¢ce √† l'optimisation m√©moire
- **CUDA Float** : Performances similaires ou l√©g√®rement meilleures que shared memory

#### Graphique 2 : Speedup (Acc√©l√©ration)

![Speedup GPU vs CPU](Part_3/plots/speedup_square_matrices.png)

Les acc√©l√©rations obtenues d√©montrent l'efficacit√© du GPU :

**Matrice 1000√ó1000√ó1000** :
- CUDA 1-thread : **16.97x** plus rapide que CPU
- CUDA Shared : **16.18x** plus rapide que CPU
- CUDA Float : **16.20x** plus rapide que CPU

**Matrice 2000√ó2000√ó2000** :
- CUDA 1-thread : **135.63x** plus rapide que CPU
- CUDA Shared : **155.76x** plus rapide que CPU
- CUDA Float : **173.66x** plus rapide que CPU (meilleure performance !)

**Observation cl√©** : Le speedup augmente avec la taille des matrices, montrant que le GPU devient encore plus avantageux pour les grandes donn√©es.

#### Graphique 3 : Comparaison de pr√©cision (Double vs Float)

![Comparaison Double vs Float](Part_3/plots/precision_comparison.png)

Ce graphique compare les performances entre pr√©cision double (64 bits) et simple (32 bits) :
- **Float est syst√©matiquement plus rapide** que double
- L'√©cart se creuse avec les grandes matrices (173x vs 155x pour N=2000)
- Trade-off int√©ressant : l√©g√®re perte de pr√©cision (7 chiffres vs 15) pour un gain de performance notable

#### Graphique 4 : Impact des optimisations CUDA

![Impact des optimisations](Part_3/plots/cuda_optimization_comparison.png)

Comparaison des diff√©rentes strat√©gies d'optimisation CUDA :
- **1-thread par bloc** : Parall√©lisme de base, bon point de d√©part
- **Shared memory** : Optimisation m√©moire, r√©duction des acc√®s globaux
- **Float precision** : Combine optimisation m√©moire + pr√©cision r√©duite = meilleure performance

Pour N=2000, la version float est **~28% plus rapide** que la version 1-thread !

#### Graphique 5 : Performance en GFLOPS

![Performance en GFLOPS](Part_3/plots/gflops_comparison.png)

Analyse du d√©bit de calcul en milliards d'op√©rations par seconde :

**Matrice 1000√ó1000√ó1000** (2 milliards d'op√©rations) :
- S√©quentiel : **2.01 GFLOPS** (CPU)
- CUDA 1-thread : **34.17 GFLOPS** (17x am√©lioration)
- CUDA Shared : **32.57 GFLOPS**
- CUDA Float : **32.62 GFLOPS**

**Matrice 2000√ó2000√ó2000** (16 milliards d'op√©rations) :
- S√©quentiel : **1.41 GFLOPS** (CPU sature)
- CUDA 1-thread : **191.55 GFLOPS** (136x am√©lioration)
- CUDA Shared : **219.98 GFLOPS**
- CUDA Float : **245.25 GFLOPS** (‚≠ê meilleure performance)

**Observation importante** : Le GPU maintient un d√©bit √©lev√© m√™me avec l'augmentation de la charge, contrairement au CPU qui plafonne.

#### Synth√®se des r√©sultats :

‚úÖ **Speedup impressionnant** : Jusqu'√† **173x** plus rapide que le CPU (version float, N=2000)

‚úÖ **Scalabilit√©** : Les performances GPU s'am√©liorent avec la taille des donn√©es

‚úÖ **Float vs Double** : Float offre le meilleur compromis performance/pr√©cision pour ce type de calcul

‚úÖ **Shared memory** : Am√©lioration modeste mais constante gr√¢ce √† l'optimisation des acc√®s m√©moire

‚ö†Ô∏è **Limite** : Tests effectu√©s uniquement sur matrices carr√©es (N=M=P) en raison de contraintes du code de r√©f√©rence

### 3.5 Corrections et Optimisations R√©alis√©es

Au cours du d√©veloppement, plusieurs corrections ont √©t√© apport√©es :

**1. Correction de l'indexation** :
- Probl√®me initial : indexation incoh√©rente entre fichiers
- Solution : uniformisation selon le mod√®le du professeur
  - `A[i*Ndim+k]` (stride = Ndim, pas Pdim !)
  - `B[k*Pdim+j]` (stride = Pdim)
  - `C[i*Ndim+j]` (stride = Ndim)

**2. Correction de la biblioth√®que half** :
- Probl√®me initial : utilisation de `cuda_fp16.h`
- Solution : passage √† `"half.hpp"` comme sp√©cifi√© dans le sujet
  - Type `__half` ‚Üí `half_float::half`
  - Conversions `__float2half()` ‚Üí `half()`
  - Plus portable et conforme au sujet

**3. Gestion des tuiles non-align√©es** :
- Ajout de v√©rifications de bornes pour matrices dont les dimensions ne sont pas multiples de 16
- √âvite les acc√®s m√©moire hors limites

### 3.6 Conclusion Partie 3

La multiplication de matrices est l'une des op√©rations les plus importantes en calcul scientifique et apprentissage automatique. Les r√©sultats exp√©rimentaux obtenus confirment la puissance du GPU pour ce type de calcul :

#### R√©sultats cl√©s obtenus :

1. **Acc√©l√©ration spectaculaire** : Jusqu'√† **173x** plus rapide que le CPU pour les grandes matrices (2000√ó2000)
   - D√©montre l'int√©r√™t majeur du GPU pour les calculs matriciels intensifs
   - Le speedup augmente avec la taille des donn√©es (scalabilit√© excellente)

2. **Float precision = meilleur choix** : 
   - **245 GFLOPS** atteints avec float vs 220 GFLOPS avec double (N=2000)
   - Gain de performance de ~11% avec une perte de pr√©cision acceptable pour la plupart des applications
   - Particuli√®rement adapt√© pour le machine learning o√π float suffit

3. **Optimisation m√©moire partag√©e** :
   - Am√©lioration constante mais mod√©r√©e (~10-20%) par rapport √† la version basique
   - Crucial pour √©viter les goulots d'√©tranglement m√©moire
   - Le tiling 16√ó16 permet de r√©utiliser efficacement les donn√©es

4. **Scalabilit√© GPU** :
   - Le CPU plafonne √† ~2 GFLOPS quelle que soit la charge
   - Le GPU maintient >200 GFLOPS m√™me avec 16 milliards d'op√©rations
   - Architecture parfaitement adapt√©e au calcul matriciel

5. **Limitations identifi√©es** :
   - Code de r√©f√©rence limit√© aux matrices carr√©es (N=M=P)
   - Version half precision non impl√©ment√©e (incompatibilit√© biblioth√®que)
   - Ces limitations n'affectent pas les conclusions g√©n√©rales

**Impact pratique** : Ces r√©sultats montrent que pour toute application n√©cessitant des multiplications de matrices de taille >1000, l'utilisation du GPU est **indispensable**. Le gain de temps est consid√©rable et se traduit directement par une productivit√© accrue en recherche et d√©veloppement.

---

## Conclusion G√©n√©rale

Ce TP a permis d'explorer en profondeur la programmation GPU avec CUDA √† travers trois applications classiques :

### Points cl√©s appris :

1. **Parall√©lisme massif** : Le GPU excelle quand on a des milliers de calculs ind√©pendants
2. **Hi√©rarchie m√©moire** : La m√©moire partag√©e et les optimisations d'acc√®s sont cruciales
3. **Trade-offs** : Pr√©cision vs vitesse, complexit√© vs performance
4. **M√©thodologie** : Importance des benchmarks et de l'analyse quantitative

### Comp√©tences acquises :

- ‚úÖ √âcriture de kernels CUDA optimis√©s
- ‚úÖ Utilisation de la m√©moire partag√©e et des r√©ductions
- ‚úÖ Gestion des diff√©rentes pr√©cisions num√©riques
- ‚úÖ Analyse de performance et calcul de speedups
- ‚úÖ Automatisation des benchmarks avec Python

### Perspectives :

Les techniques apprises sont directement applicables √† :
- Deep Learning (multiplication de matrices omnipr√©sente)
- Calcul scientifique (simulations physiques)
- Traitement d'images (convolutions)
- Analyse de donn√©es (op√©rations vectorielles)

Le GPU n'est plus une option mais une n√©cessit√© pour le calcul haute performance moderne ! üöÄ

---

**Fin du rapport**