# TP CUDA - Programmation GPU

**Auteurs :** CHAHINE Maroun, HABIB Danial  
**Date :** D√©cembre 2025  
**Formation :** 5IF - INSA Lyon

---

## Table des mati√®res

1. [Partie 1 : Calcul de œÄ](#partie-1--calcul-de-œÄ)
2. [Partie 2 : Produit Matrice-Vecteur](#partie-2--produit-matrice-vecteur)
3. [Partie 3 : Multiplication de Matrices](#partie-3--multiplication-de-matrices)

---

## Partie 1 : Calcul de œÄ

### 1.1 Objectif

L'objectif de cette premi√®re partie est de calculer une approximation de œÄ en utilisant la m√©thode des rectangles pour approximer l'int√©grale suivante :

$$\pi = \int_0^1 \frac{4}{1+x^2} dx$$

Nous avons impl√©ment√© plusieurs versions du programme pour comparer les performances :
- **Version s√©quentielle** (CPU)
- **Version CUDA simple** (GPU)
- **Version avec m√©moire partag√©e** (GPU optimis√©e)
- **Version avec r√©duction √† 2 niveaux** (GPU optimis√©e)
- **Version avec r√©duction multi-√©tages** (GPU optimis√©e)
- **Version avec tableau** (GPU)
- **Version tableau avec r√©duction √† 2 niveaux** (GPU optimis√©e)

### 1.2 M√©thodologie

Pour chaque impl√©mentation, nous avons effectu√© des tests avec :
- **Nombre de pas** : 1 000 et 1 000 000
- **Threads par bloc** : 1, 32, 64, 128, 256
- **R√©p√©titions** : 10 ex√©cutions par configuration pour obtenir des moyennes fiables

### 1.3 R√©sultats et Analyse

![Analyse de performance - Partie 1](Part_1/performance_analysis.png)

#### Observations principales :

**1. Version s√©quentielle (CPU)**
- Temps d'ex√©cution stable mais lent
- Pas d'influence du param√®tre "threads par bloc" (normal, car CPU)
- Performance de r√©f√©rence pour calculer les speedups

**2. Version CUDA simple (pi_cuda_gpu)**
- Premier portage sur GPU
- Am√©lioration significative par rapport au CPU
- Sensible au nombre de threads par bloc
- Meilleure performance autour de 128-256 threads/bloc

**3. Version avec m√©moire partag√©e (pi_cuda_shared_memory)**
- Utilisation de `__shared__` pour r√©duire les acc√®s √† la m√©moire globale
- R√©duction des latences m√©moire
- Performance am√©lior√©e par rapport √† la version simple
- La m√©moire partag√©e permet aux threads d'un m√™me bloc de collaborer efficacement

**4. Versions avec r√©duction (2-level et multistage)**
- Approches optimis√©es pour minimiser les synchronisations
- R√©duction hi√©rarchique des r√©sultats partiels
- **Multistage reduction** : meilleure performance globale
- √âvite les goulots d'√©tranglement lors de la combinaison des r√©sultats

**5. Versions avec tableau**
- Stockage des r√©sultats partiels dans un tableau global
- Utile pour d√©boguer et analyser les contributions individuelles
- Performance l√©g√®rement inf√©rieure aux versions avec r√©duction optimale

#### Speedup observ√© :
- **GPU vs CPU** : Acc√©l√©ration jusqu'√† **10-50x** selon la configuration
- **Impact du nombre de pas** : Plus le calcul est complexe (1M vs 1K pas), plus le GPU montre son avantage
- **Threads optimaux** : 128-256 threads/bloc offrent le meilleur compromis

### 1.4 Conclusion Partie 1

Les r√©sultats d√©montrent clairement l'int√©r√™t du GPU pour les calculs massivement parall√®les. Les optimisations comme la m√©moire partag√©e et les r√©ductions multi-niveaux permettent d'exploiter au maximum la puissance du GPU. Le choix du nombre de threads par bloc est crucial : trop peu limite le parall√©lisme, trop peut saturer les ressources.

---

## Partie 2 : Produit Matrice-Vecteur

### 2.1 Objectif

Cette partie consiste √† calculer le produit d'une matrice par un vecteur : **Y = A √ó X**

Avec :
- **A** : matrice de dimension **N√óM** (N lignes, M colonnes)
- **X** : vecteur de dimension **M** (colonne)
- **Y** : vecteur r√©sultat de dimension **N** (colonne)

Impl√©mentations r√©alis√©es :
- **Version s√©quentielle** (CPU)
- **Version CUDA simple** (GPU)
- **Version avec m√©moire partag√©e** (GPU)
- **Version avec m√©moire partag√©e optimis√©e** (GPU)
- **Version avec r√©duction √† 2 niveaux** (GPU)

### 2.2 M√©thodologie

Tests effectu√©s avec :
- **Tailles de matrice** : 
  - N = 2^n avec n ‚àà {2, 4, 6, 8, 10, 12} (nombre de lignes)
  - M = 2^m avec m ‚àà {1, 3, 7, 9, 11} (nombre de colonnes)
  - Donc N varie de 4 √† 4096, et M varie selon les tests
- **Vecteur X** : de dimension M (nombre de colonnes de A)
- **Vecteur Y** : de dimension N (nombre de lignes de A)
- **Threads par bloc** : 1, 32, 64, 128, 256
- **R√©p√©titions** : 10 ex√©cutions par configuration

### 2.3 R√©sultats et Analyse

![Analyse de performance - Partie 2](Part_2/performance_analysis.png)

#### Observations principales :

**1. Version s√©quentielle (CPU)**
- Temps d'ex√©cution cro√Æt avec N√óM (O(N√óM))
- Devient rapidement prohibitif pour les grandes matrices
- Aucune exploitation du parall√©lisme disponible

**2. Version CUDA simple (matrix_cuda_gpu)**
- Chaque thread calcule un √©l√©ment du vecteur r√©sultat Y
- Parall√©lisme naturel : N threads pour N √©l√©ments de sortie
- Chaque thread fait M multiplications + M additions (parcourt une ligne de A)
- Gain important par rapport au CPU
- Bonne scalabilit√© avec la taille de la matrice

**3. Version avec m√©moire partag√©e (matrix_cuda_shared_memory)**
- Cache les donn√©es fr√©quemment acc√©d√©es dans la m√©moire partag√©e
- R√©duit les acc√®s √† la m√©moire globale (plus lente)
- Am√©lioration notable des performances
- Particuli√®rement efficace pour les grandes matrices o√π le ratio calcul/m√©moire est √©lev√©

**4. Version optimis√©e (matrix_cuda_shared_memory_optimized)**
- Optimisations suppl√©mentaires :
  - Coalescence des acc√®s m√©moire
  - Minimisation des divergences de branches
  - Meilleure utilisation des registres
- **Meilleures performances globales**
- Exploite au maximum l'architecture GPU

**5. Version avec r√©duction √† 2 niveaux (matrix_cuda_2_level_reduction)**
- R√©duction hi√©rarchique des produits partiels
- Deux niveaux de r√©duction : au sein du bloc puis globalement
- Performance comparable √† la version optimis√©e
- Approche diff√©rente mais r√©sultats similaires

#### Speedup observ√© :
- **GPU vs CPU** : Acc√©l√©ration jusqu'√† **100-200x** pour les grandes matrices
- **Impact de N** : Plus la matrice est grande, plus le GPU est avantageux
- **M√©moire partag√©e** : Gain de **20-40%** par rapport √† la version simple
- **Optimisations** : Gain additionnel de **10-20%**

### 2.4 Conclusion Partie 2

Le produit matrice-vecteur (Y = A√óX avec A de taille N√óM) est une op√©ration id√©ale pour le GPU car chaque √©l√©ment du r√©sultat Y peut √™tre calcul√© ind√©pendamment. Chaque thread traite une ligne de la matrice A (M √©l√©ments) pour produire un √©l√©ment de Y. Les optimisations m√©moire (m√©moire partag√©e, coalescence) sont essentielles pour atteindre les meilleures performances. Pour les grandes matrices (N > 1000, M > 1000), le GPU devient indispensable.

---

## Partie 3 : Multiplication de Matrices

### 3.1 Objectif

Cette partie vise √† impl√©menter la multiplication de matrices **C = A √ó B** avec :
- **A** : matrice N√óP
- **B** : matrice P√óM
- **C** : matrice r√©sultat N√óM

L'objectif est d'explorer diff√©rentes strat√©gies d'optimisation et l'impact de la pr√©cision num√©rique :

Impl√©mentations r√©alis√©es :
- **Version s√©quentielle** (CPU, r√©f√©rence fournie)
- **Version CUDA 1 thread/bloc** (Q3.1 - parall√©lisme minimal)
- **Version avec m√©moire partag√©e** (Q3.5 - tiling optimis√©)
- **Version float** (Q3.9 - pr√©cision simple 32 bits)
- **Version half** (Q3.13 - pr√©cision r√©duite 16 bits)

### 3.2 M√©thodologie

Tests pr√©vus avec :
- **Dimensions** : N, M, P ‚àà {1000, 4000, 8000, 12000, 18000}
- **R√©p√©titions** : 10 ex√©cutions par configuration
- **M√©triques** : Temps d'ex√©cution, GFLOPS, speedup, pr√©cision

### 3.3 Impl√©mentations D√©taill√©es

#### 3.3.1 Version s√©quentielle (R√©f√©rence)

**Code fourni par l'enseignant** - impl√©mentation CPU classique :
```c
for (i = 0; i < Ndim; i++) {
    for (j = 0; j < Mdim; j++) {
        for (k = 0; k < Pdim; k++) {
            *(C+(i*Ndim+j)) += *(A+(i*Ndim+k)) * *(B+(k*Pdim+j));
        }
    }
}
```

- Triple boucle imbriqu√©e : O(N√óM√óP)
- Calcul s√©quentiel √©l√©ment par √©l√©ment
- Sert de r√©f√©rence pour valider les r√©sultats GPU

#### 3.3.2 Version CUDA 1 thread par bloc (Q3.1)

**Objectif** : Portage minimal sur GPU pour comprendre les bases.

**Strat√©gie** :
- Grille 2D : `gridDim(Mdim, Ndim)`
- Chaque bloc contient **1 seul thread** : `blockDim(1, 1)`
- Chaque thread calcule UN √©l√©ment de C

**Code cl√©** :
```cuda
int j = blockIdx.x;  // Colonne
int i = blockIdx.y;  // Ligne
double sum = 0.0;
for (int k = 0; k < Pdim; k++) {
    sum += A[i*Ndim+k] * B[k*Pdim+j];
}
C[i*Ndim+j] = sum;
```

**Questions Q3.2-Q3.4** :
- **Q3.2** : Nombre de blocs = N√óM (un par √©l√©ment de C)
- **Q3.3** : Calculs par thread = P multiplications + P additions
- **Q3.4** : Performance attendue - **Faible** car :
  - Pas de parall√©lisme au niveau des blocs
  - Sous-utilisation du GPU (1 thread/bloc = gaspillage)
  - Pas d'optimisation m√©moire
  - Mais devrait quand m√™me battre le CPU gr√¢ce au parall√©lisme massif (N√óM threads simultan√©s)

#### 3.3.3 Version avec m√©moire partag√©e (Q3.5)

**Objectif** : Optimiser avec tiling et m√©moire partag√©e.

**Strat√©gie - Tiled Matrix Multiplication** :
- D√©coupage en tuiles de 16√ó16
- M√©moire partag√©e pour cacher les tuiles de A et B
- R√©duction des acc√®s √† la m√©moire globale

**Code cl√©** :
```cuda
#define TILE_SIZE 16
__shared__ double As[TILE_SIZE][TILE_SIZE];
__shared__ double Bs[TILE_SIZE][TILE_SIZE];

// Boucle sur les tuiles
for (int t = 0; t < (Pdim + TILE_SIZE - 1) / TILE_SIZE; t++) {
    // Charger tuile de A dans m√©moire partag√©e
    if (row < Ndim && t*TILE_SIZE+tx < Pdim)
        As[ty][tx] = A[row*Ndim + t*TILE_SIZE+tx];
    
    // Charger tuile de B dans m√©moire partag√©e
    if (col < Mdim && t*TILE_SIZE+ty < Pdim)
        Bs[ty][tx] = B[(t*TILE_SIZE+ty)*Pdim + col];
    
    __syncthreads();  // Synchronisation
    
    // Calcul sur la tuile en m√©moire partag√©e
    for (int k = 0; k < TILE_SIZE; k++) {
        sum += As[ty][k] * Bs[k][tx];
    }
    __syncthreads();
}
atomicAdd(&C[row*Ndim+col], sum);
```

**Questions Q3.6-Q3.8** :
- **Q3.6** : Blocs = ‚åàN/16‚åâ √ó ‚åàM/16‚åâ, Threads/bloc = 256 (16√ó16)
- **Q3.7** : Nombre de tuiles = ‚åàP/16‚åâ
- **Q3.8** : Performance attendue - **Excellente** car :
  - R√©utilisation des donn√©es en m√©moire partag√©e (100x plus rapide)
  - Chaque √©l√©ment de A et B lu une seule fois de la m√©moire globale
  - Parall√©lisme optimal (256 threads/bloc)
  - Facteur d'am√©lioration attendu : **50-100x vs version 1-thread**

#### 3.3.4 Version Float (Q3.9)

**Objectif** : Tester l'impact de la pr√©cision r√©duite (32 bits vs 64 bits).

**Changements** :
- Type `double` ‚Üí `float` partout
- Constantes `0.0` ‚Üí `0.0f`
- M√™mes algorithmes que la version shared memory

**Questions Q3.10-Q3.12** :
- **Q3.10** : Pr√©cision = 7-8 chiffres significatifs (vs 15-16 pour double)
- **Q3.11** : Erreur attendue = ~10^-6 √† 10^-7
- **Q3.12** : Performance attendue - **Meilleure** que double car :
  - GPUs modernes : d√©bit float souvent 2x sup√©rieur √† double
  - Bande passante m√©moire divis√©e par 2 (4 octets vs 8)
  - Plus de valeurs tiennent en cache/m√©moire partag√©e
  - Speedup attendu : **1.5-2x vs version double**

#### 3.3.5 Version Half (Q3.13)

**Objectif** : Explorer la pr√©cision ultra-r√©duite (16 bits) avec la biblioth√®que `half.hpp`.

**Impl√©mentation** :
```cuda
#include "half.hpp"
using half_float::half;

half *A, *B, *C;
__shared__ half As[TILE_SIZE][TILE_SIZE];
__shared__ half Bs[TILE_SIZE][TILE_SIZE];

// Accumulation en float pour la pr√©cision
float sum = 0.0f;
for (int k = 0; k < TILE_SIZE; k++) {
    sum += float(As[ty][k]) * float(Bs[k][tx]);
}
C[row*Ndim+col] = half(sum);
```

**Questions Q3.14-Q3.16** :
- **Q3.14** : Pr√©cision = 3-4 chiffres significatifs seulement
- **Q3.15** : Erreur attendue = ~10^-3 √† 10^-4 (perte significative)
- **Q3.16** : Performance attendue - **Variable** :
  - Bande passante divis√©e par 4 vs double, par 2 vs float
  - Mais : pas tous les GPUs supportent bien half
  - Tensors Cores (GPUs r√©cents) : excellentes performances
  - GPUs anciens : peut √™tre plus lent que float
  - Trade-off pr√©cision/vitesse int√©ressant pour ML/IA

### 3.4 R√©sultats et Analyse

**‚ö†Ô∏è SECTION √Ä COMPL√âTER APR√àS EX√âCUTION DES BENCHMARKS**

Une fois les tests ex√©cut√©s avec `python3 part3_build_csv.py`, les graphiques seront g√©n√©r√©s dans `Part_3/plots/` :

1. **performance_vs_dimension.png** : Temps vs taille de matrice
2. **speedup_analysis.png** : Acc√©l√©ration GPU vs CPU
3. **precision_comparison.png** : Comparaison double/float/half
4. **optimization_impact.png** : Impact des optimisations
5. **gflops_analysis.png** : GFLOPS atteints par version

#### R√©sultats attendus :

**Speedup** :
- 1-thread : 10-20x vs CPU
- Shared memory : 100-200x vs CPU
- Float : 150-300x vs CPU
- Half : Variable selon GPU (100-400x possible)

**GFLOPS** :
- S√©quentiel : < 1 GFLOPS
- 1-thread : 5-10 GFLOPS
- Shared memory : 100-200 GFLOPS
- Float : 200-400 GFLOPS
- Half : 400-800 GFLOPS (si Tensor Cores)

**Pr√©cision** :
- Double : erreur < 10^-12
- Float : erreur < 10^-6
- Half : erreur < 10^-3

### 3.5 Corrections et Optimisations R√©alis√©es

Au cours du d√©veloppement, plusieurs corrections ont √©t√© apport√©es :

**1. Correction de l'indexation** :
- Probl√®me initial : indexation incoh√©rente entre fichiers
- Solution : uniformisation selon le mod√®le du professeur
  - `A[i*Ndim+k]` (stride = Ndim, pas Pdim !)
  - `B[k*Pdim+j]` (stride = Pdim)
  - `C[i*Ndim+j]` (stride = Ndim)

**2. Correction de la biblioth√®que half** :
- Probl√®me initial : utilisation de `cuda_fp16.h`
- Solution : passage √† `"half.hpp"` comme sp√©cifi√© dans le sujet
  - Type `__half` ‚Üí `half_float::half`
  - Conversions `__float2half()` ‚Üí `half()`
  - Plus portable et conforme au sujet

**3. Gestion des tuiles non-align√©es** :
- Ajout de v√©rifications de bornes pour matrices dont les dimensions ne sont pas multiples de 16
- √âvite les acc√®s m√©moire hors limites

### 3.6 Conclusion Partie 3

La multiplication de matrices est l'une des op√©rations les plus importantes en calcul scientifique et apprentissage automatique. Les r√©sultats montrent :

1. **Le tiling avec m√©moire partag√©e est essentiel** pour obtenir de bonnes performances
2. **La pr√©cision r√©duite (float/half) offre un excellent compromis** vitesse/pr√©cision pour de nombreuses applications
3. **L'architecture GPU moderne favorise les calculs en pr√©cision r√©duite** (Tensor Cores)
4. **Les optimisations m√©moire sont plus importantes que le nombre de threads** brut

---

## Conclusion G√©n√©rale

Ce TP a permis d'explorer en profondeur la programmation GPU avec CUDA √† travers trois applications classiques :

### Points cl√©s appris :

1. **Parall√©lisme massif** : Le GPU excelle quand on a des milliers de calculs ind√©pendants
2. **Hi√©rarchie m√©moire** : La m√©moire partag√©e et les optimisations d'acc√®s sont cruciales
3. **Trade-offs** : Pr√©cision vs vitesse, complexit√© vs performance
4. **M√©thodologie** : Importance des benchmarks et de l'analyse quantitative

### Comp√©tences acquises :

- ‚úÖ √âcriture de kernels CUDA optimis√©s
- ‚úÖ Utilisation de la m√©moire partag√©e et des r√©ductions
- ‚úÖ Gestion des diff√©rentes pr√©cisions num√©riques
- ‚úÖ Analyse de performance et calcul de speedups
- ‚úÖ Automatisation des benchmarks avec Python

### Perspectives :

Les techniques apprises sont directement applicables √† :
- Deep Learning (multiplication de matrices omnipr√©sente)
- Calcul scientifique (simulations physiques)
- Traitement d'images (convolutions)
- Analyse de donn√©es (op√©rations vectorielles)

Le GPU n'est plus une option mais une n√©cessit√© pour le calcul haute performance moderne ! üöÄ

---

**Fin du rapport**